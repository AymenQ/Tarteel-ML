{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import tensorflow as tf\n",
    "import dill as pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Preprocess Audio Files in Surah Fatihah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take quite a bit of time, but the good news is that you only need to do it once! After you've done this once, the files will be saved locally and you can skip the cells in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../download.py\" -s 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_features.py\" -f mfcc -s 1 --local_download_dir \"../.audio\" --output_dir \"../.outputs\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_one_hot_encoding.py\" -i \"../data/data-uthmani.json\" -o \"../data/one-hot.pkl\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Methods to Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\"\"\"\n",
    "\n",
    "def convert_list_of_arrays_to_padded_array(list_varying_sizes, pad_value=0):\n",
    "    '''\n",
    "    Converts a list of arrays of varying sizes to a single numpy array. The extra elements are set to 0\n",
    "    '''\n",
    "    max_shape = [0]*len(list_varying_sizes[0].shape)\n",
    "    # first pass to compute the max size\n",
    "    for arr in list_varying_sizes:\n",
    "        shape = arr.shape\n",
    "        max_shape = [max(s1, s2) for s1, s2 in zip(shape, max_shape)]\n",
    "    padded_array = pad_value * np.ones((len(list_varying_sizes), *max_shape))\n",
    "    \n",
    "    # second pass to fill in the values in the array:\n",
    "    for a, arr in enumerate(list_varying_sizes):\n",
    "        r, c = arr.shape  # TODO(abidlabs): maybe make more general to more than just 2D arrays.\n",
    "        padded_array[a, :r, :c] = arr\n",
    "    \n",
    "    return padded_array\n",
    "\n",
    "def preprocess_encoder_input(arr):\n",
    "    '''\n",
    "    Simple method to handle the complex MFCC coefs that are produced during preprocessing. This means:\n",
    "    1. (For now), discarding one of the channels of the MFCC coefs\n",
    "    2. Collapsing any empty dimensions\n",
    "    '''\n",
    "    return arr.squeeze()[0]\n",
    "\n",
    "\n",
    "# Load every one-hot-encoded output as a dictionary\n",
    "with open('../data/one-hot.pkl', 'rb') as one_hot_quran_pickle_file:\n",
    "    one_hot_obj = pickle.load(one_hot_quran_pickle_file)\n",
    "\n",
    "\n",
    "def get_one_hot_encoded_verse(surah_num, ayah_num): \n",
    "    '''\n",
    "    Converts a one-hot-encoded verse into forms that can be used by the LSTM decoder\n",
    "    \n",
    "    :param surah_num: an int designating the chapter number, one-indexed\n",
    "    :param ayah_num: an int designating the verse number, one-indexed\n",
    "    '''\n",
    "    # Load the preprocessed one-hot encoding \n",
    "    one_hot_verse = one_hot_obj['quran']['surahs'][surah_num - 1]['ayahs'][ayah_num - 1]['text']\n",
    "    num_chars_in_verse, num_unique_chars = one_hot_verse.shape\n",
    "    \n",
    "    # Generate decoder_input_data \n",
    "    decoder_input = np.zeros((num_chars_in_verse+2, num_unique_chars+2))\n",
    "    decoder_input[0, :] = [0] * num_unique_chars + [1, 0] # START token\n",
    "    decoder_input[1:num_chars_in_verse+1, :-2] = one_hot_verse # original verse\n",
    "    decoder_input[-1, :] = [0] * num_unique_chars + [0, 1] # STOP token\n",
    "\n",
    "    # Generate decoder_target_data \n",
    "    decoder_target = np.zeros((num_chars_in_verse+2, num_unique_chars+2))\n",
    "    decoder_target[:num_chars_in_verse, :-2] = one_hot_verse # original verse\n",
    "    decoder_target[-1, :] = [0] * num_unique_chars + [0, 1] # STOP token\n",
    "    \n",
    "    return decoder_input, decoder_target\n",
    "\n",
    "    \n",
    "def build_dataset(local_coefs_dir='../.outputs/mfcc', surahs=[1], n=100):\n",
    "    '''\n",
    "    Builds a dataset to be used with the sequence-to-sequence network.\n",
    "    \n",
    "    :param local_coefs_dir: a string with the path of the coefficients for prediction\n",
    "    '''\n",
    "    \n",
    "    def get_encoder_and_decoder_data(n=100):\n",
    "        count = 0\n",
    "        encoder_input_data = []\n",
    "        decoder_input_data = []\n",
    "        decoder_target_data = []\n",
    "        for surah_num in surahs:\n",
    "            local_surah_dir = os.path.join(local_coefs_dir, \"s\" + str(surah_num))\n",
    "            for _, ayah_directories, _ in os.walk(local_surah_dir):\n",
    "                for ayah_directory in ayah_directories:\n",
    "                    ayah_num = ayah_directory[1:]\n",
    "                    local_ayah_dir = os.path.join(local_surah_dir, ayah_directory)\n",
    "                    for _, _, recording_filenames in os.walk(local_ayah_dir):\n",
    "                        for recording_filename in recording_filenames:\n",
    "                            local_coefs_path = os.path.join(local_ayah_dir, recording_filename)\n",
    "                            encoder_input = np.load(local_coefs_path)\n",
    "                            encoder_input = preprocess_encoder_input(encoder_input)\n",
    "                            encoder_input_data.append(encoder_input)\n",
    "\n",
    "                            decoder_input, decoder_target = get_one_hot_encoded_verse(int(surah_num), int(ayah_num))\n",
    "                            decoder_input_data.append(decoder_input)\n",
    "                            decoder_target_data.append(decoder_target)\n",
    "                            count += 1\n",
    "                            if count == n:\n",
    "                                return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "    \n",
    "    \n",
    "    encoder_input_data, decoder_input_data, decoder_target_data = get_encoder_and_decoder_data(n=n)\n",
    "    encoder_input_data = convert_list_of_arrays_to_padded_array(encoder_input_data)\n",
    "    decoder_input_data = convert_list_of_arrays_to_padded_array(decoder_input_data)\n",
    "    decoder_target_data = convert_list_of_arrays_to_padded_array(decoder_target_data)\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = build_dataset(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 914, 13)\n",
      "(10, 40, 61)\n",
      "(10, 40, 61)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(a.shape) for a in [encoder_input_data, decoder_input_data, decoder_target_data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_one_hot_encoded_verse(1, 1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
