{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dill as pickle\n",
    "import os\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Preprocess Audio Files in Surah Fatihah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take quite a bit of time, but the good news is that you only need to do it once! After you've done this once, the files will be saved locally and you can skip the cells in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../download.py\" -s 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_features.py\" -f mfcc -s 1 --local_download_dir \"../.audio\" --output_dir \"../.outputs\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../audio_preprocessing/generate_one_hot_encoding.py\" -i \"../data/data-uthmani.json\" -o \"../data/one-hot.pkl\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Methods to Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by: https://github.com/ruohoruotsi/LSTM-Music-Genre-Classification/blob/master/lstm_genre_classifier_keras.py\n",
    "\"\"\"\n",
    "\n",
    "def convert_list_of_arrays_to_padded_array(list_varying_sizes, pad_value=0):\n",
    "    '''\n",
    "    Converts a list of arrays of varying sizes to a single numpy array. The extra elements are set to 0\n",
    "    '''\n",
    "    max_shape = [0]*len(list_varying_sizes[0].shape)\n",
    "    # first pass to compute the max size\n",
    "    for arr in list_varying_sizes:\n",
    "        shape = arr.shape\n",
    "        max_shape = [max(s1, s2) for s1, s2 in zip(shape, max_shape)]\n",
    "    padded_array = pad_value * np.ones((len(list_varying_sizes), *max_shape))\n",
    "    \n",
    "    # second pass to fill in the values in the array:\n",
    "    for a, arr in enumerate(list_varying_sizes):\n",
    "        r, c = arr.shape  # TODO(abidlabs): maybe make more general to more than just 2D arrays.\n",
    "        padded_array[a, :r, :c] = arr\n",
    "    \n",
    "    return padded_array\n",
    "\n",
    "\n",
    "def preprocess_encoder_input(arr):\n",
    "    '''\n",
    "    Simple method to handle the complex MFCC coefs that are produced during preprocessing. This means:\n",
    "    1. (For now), discarding one of the channels of the MFCC coefs\n",
    "    2. Collapsing any empty dimensions\n",
    "    '''\n",
    "    return arr.squeeze()[0]\n",
    "\n",
    "    \n",
    "# Load the CSV file with URLs and Gender information\n",
    "with open('../.cache/tarteel_v1.0.csv', 'rb') as tarteel_csv:\n",
    "    tarteel_df = pd.read_csv(tarteel_csv)\n",
    "    tarteel_df = tarteel_df[['URL to Recording', 'Gender']]\n",
    "    recording_urls = tarteel_df['URL to Recording']\n",
    "    recording_filenames_from_csv = recording_urls.str.extract('https://tarteel-data.s3.amazonaws.com/media/([_\\d]+)\\.wav.+')\n",
    "    \n",
    "    \n",
    "def get_gender_of_recitation(recording_filename):\n",
    "    recording_filename = recording_filename[:-10]\n",
    "    matching_audio_file = recording_filenames_from_csv[0].str.startswith(recording_filename).fillna(False)\n",
    "    tarteel_df_out = tarteel_df[matching_audio_file]\n",
    "    if tarteel_df_out.empty:\n",
    "        return None\n",
    "    if tarteel_df_out['Gender'].iloc[0] == 'male':\n",
    "        return np.array([1, 0])\n",
    "    if tarteel_df_out['Gender'].iloc[0] == 'female':\n",
    "        return np.array([0, 1])\n",
    "    return None\n",
    "    \n",
    "\n",
    "def build_dataset(local_coefs_dir='../.outputs/mfcc', surahs=[1], n=100):\n",
    "    '''\n",
    "    Builds a dataset to be used with the sequence-to-sequence network.\n",
    "    \n",
    "    :param local_coefs_dir: a string with the path of the coefficients for prediction\n",
    "    '''\n",
    "    \n",
    "    def get_encoder_and_decoder_data(n=100):\n",
    "        count = 0\n",
    "        encoder_input_data = []\n",
    "        gender_data = []\n",
    "        for surah_num in surahs:\n",
    "            local_surah_dir = os.path.join(local_coefs_dir, \"s\" + str(surah_num))\n",
    "            for _, ayah_directories, _ in os.walk(local_surah_dir):\n",
    "                for ayah_directory in ayah_directories:\n",
    "                    ayah_num = ayah_directory[1:]\n",
    "                    local_ayah_dir = os.path.join(local_surah_dir, ayah_directory)\n",
    "                    for _, _, recording_filenames in os.walk(local_ayah_dir):\n",
    "                        for recording_filename in recording_filenames:\n",
    "                            local_coefs_path = os.path.join(local_ayah_dir, recording_filename)\n",
    "                            encoder_input = np.load(local_coefs_path)\n",
    "                            encoder_input = preprocess_encoder_input(encoder_input)\n",
    "\n",
    "                            gender = get_gender_of_recitation(recording_filename)\n",
    "                            if gender is not None:  # Only if gender is known, add the recording to the list\n",
    "                                encoder_input_data.append(encoder_input)\n",
    "                                gender_data.append(gender)\n",
    "                                count += 1\n",
    "                                if count == n:\n",
    "                                    return encoder_input_data, gender_data\n",
    "\n",
    "        return encoder_input_data, gender_data\n",
    "    \n",
    "    \n",
    "    encoder_input_data, gender_data = get_encoder_and_decoder_data(n=n)\n",
    "    encoder_input_data = convert_list_of_arrays_to_padded_array(encoder_input_data)\n",
    "    gender_data = np.stack(gender_data)\n",
    "    return encoder_input_data, gender_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching_audio_file = recording_filenames_from_csv[0].str.startswith('1_1').fillna(False)\n",
    "# tarteel_df_out = tarteel_df[matching_audio_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # Batch size for training.\n",
    "epochs = 25  # Number of epochs to train for.\n",
    "n_units_1 = 128  # number of LSTM cells in layer 1\n",
    "n_units_2 = 32 # number of LSTM cells in layer 2\n",
    "n = 100\n",
    "\n",
    "encoder_input_data, gender_data = build_dataset(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(a.shape) for a in [encoder_input_data, gender_data]]\n",
    "\n",
    "input_shape = encoder_input_data.shape[1], encoder_input_data.shape[2]\n",
    "num_classes = gender_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we tried to create a dataset with 100 recordings, only 30 of them had gender recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Keras Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=n_units_1, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\n",
    "model.add(LSTM(units=n_units_2, dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(encoder_input_data, gender_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), history.history['loss'])\n",
    "plt.plot(range(epochs), history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drop in loss curves suggest that the model is learning something. It seems to have started to overfit, likely because our training set is so small, that it's picking up on irrelevant patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
